---
title: "Publications"
date: 2025-11-18
description: "Research publications and academic work"
showDate: false
showAuthor: false
showReadingTime: false
---

## Conference Publications

### Unveiling Vulnerability of Self-Attention
**Khai Jiet Liong**, Hongqiu Wu, Hai Zhao

*COLING 2024 - The 30th International Conference on Computational Linguistics*

This work investigates the vulnerabilities in self-attention mechanisms, particularly in the context of adversarial attacks on natural language processing models. We present novel insights into how self-attention layers can be exploited and propose methods to understand their robustness.

[ðŸ“„ PDF](/data/2402.16470.pdf)

---

## Research Focus

My research at Shanghai Jiao Tong University's BCMI Lab focused on:

- **Adversarial Attacks on NLP Models**: Investigating vulnerabilities in transformer-based architectures and self-attention mechanisms
- **Model Robustness**: Understanding and improving the robustness of neural language models
- **Natural Language Processing**: Exploring the intersection of model interpretability and security in NLP systems

---

## Academic Background

**Master of Science in Computer Science**
Shanghai Jiao Tong University, China
*September 2020 - March 2024*
GPA: 3.18/4.0
Advisor: Prof. Hai Zhao
Research Area: Natural Language Processing, Adversarial Attack
